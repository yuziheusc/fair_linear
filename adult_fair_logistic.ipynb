{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy.linalg\n",
    "import scipy \n",
    "import scipy.linalg\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuzi/.local/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0, './utils')\n",
    "#import evaluate_utils\n",
    "from utils import evaluate_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.fair_var import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(x,y):\n",
    "    return np.corrcoef(x, y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_proba(x0):\n",
    "    x1 = np.minimum(x0,+400)\n",
    "    x2 = np.maximum(x1,-400)\n",
    "    return 1./(1. + np.exp(-x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fair_logestic_regression(df_train, df_test, feature_list, protect_list, outcome, lbd, hps):\n",
    "    ## this function will perfrom a logestic regression\n",
    "    ## df_train, df_test : two dataframes.\n",
    "    ## feature_list : should include protected features\n",
    "    ## protect : a list of protected features\n",
    "    ## outcome : a column name for the outcome column\n",
    "    ## lbd: a real number, lambda factor\n",
    "    ##\n",
    "    ## Return : predicting scores on test data\n",
    "    \n",
    "    ## create the fair features\n",
    "    fair_feature_list = []\n",
    "    df_train_tmp = pd.DataFrame()\n",
    "    df_test_tmp = pd.DataFrame()\n",
    "    for column in feature_list:\n",
    "        if(column in protect_list): continue\n",
    "        column_fair = column + \"_fair\"\n",
    "        fair_feature_list.append(column_fair)\n",
    "        df_train_tmp[column_fair] = gen_latent_nonparam_regula(df_train[feature_list], protect_list, column, lbd)\n",
    "        df_test_tmp[column_fair] = gen_latent_nonparam_regula(df_test[feature_list], protect_list, column, lbd)\n",
    "        #print(\"corr train = \", get_corr(df_train[column], df_train[protect_list[0]]))\n",
    "        #print(\"corr test = \",  get_corr(df_train[column], df_train[protect_list[0]]))\n",
    "        \n",
    "    ## add protect to the tmp dataframe\n",
    "    for column in protect_list:\n",
    "        df_train_tmp[column] = df_train[column].values\n",
    "        df_test_tmp[column] = df_test[column].values\n",
    "    \n",
    "    ## add the outcome to tmp dataframe\n",
    "    df_train_tmp[outcome] = df_train[outcome].values\n",
    "    df_test_tmp[outcome] = df_test[outcome].values\n",
    "    \n",
    "    \n",
    "    ## logestic regression on train data\n",
    "    X_train = df_train_tmp[fair_feature_list]\n",
    "    X_train = X_train.values\n",
    "    \n",
    "    Y_train = df_train_tmp[outcome].astype('int')\n",
    "    Y_train = Y_train.values\n",
    "    \n",
    "    if(False):\n",
    "        print(\"hyper parameters = \", hps)\n",
    "\n",
    "    \n",
    "    ## build random forest model\n",
    "    #clf = RandomForestClassifier(n_estimators=hps[\"n_estimators\"],random_state=0)\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l2', C=hps[\"C\"], solver='newton-cg', max_iter=200)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    X_test = df_test_tmp[fair_feature_list]\n",
    "    X_test = X_test.values\n",
    "    \n",
    "    if(False):\n",
    "        for i in range(X_train.shape[1]):\n",
    "            print(\"Train feature corr = \", get_corr(X_train[:,i], df_train[protect_list[0]]))\n",
    "            print(\"Test feature corr = \", get_corr(X_test[:,i], df_test[protect_list[0]]))\n",
    "            \n",
    "    \n",
    "    #Ypred_r_train = clf.predict_proba(X_train)\n",
    "    #Ypred_r_test = clf.predict_proba(X_test)\n",
    "    Ypred_l_train = clf.decision_function(X_train)\n",
    "    Ypred_l_test = clf.decision_function(X_test)\n",
    "    Ypred_r_train = trans_proba(Ypred_l_train)\n",
    "    Ypred_r_test = trans_proba(Ypred_l_test)\n",
    "    \n",
    "    return Ypred_r_train, Ypred_r_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data_path, test_folder, feature_list, protect_list, outcome, lbd, hps, n_fold, result_name):\n",
    "    df_res_test = pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_fold):\n",
    "        path_train = data_path+\"train/trans_train-%d.csv\"%(i+1)\n",
    "        path_test = None\n",
    "        if(test_folder == \"valid\"): \n",
    "            path_test = data_path+\"valid/trans_valid-%d.csv\"%(i+1)\n",
    "        elif(test_folder == \"test\"):                \n",
    "            path_test = data_path+\"test/trans_test-%d.csv\"%(i+1)\n",
    "        else:\n",
    "            raise Exception(\"Unkonwn test folder = %s\"%(test_folder))\n",
    "        \n",
    "        df_i_test = pd.DataFrame()\n",
    "        \n",
    "        df_train = pd.read_csv(path_train)\n",
    "        df_test = pd.read_csv(path_test)\n",
    "            \n",
    "        score_train, score_test = do_fair_logestic_regression(df_train, df_test, feature_list, protect_list, outcome, lbd, hps)    \n",
    "        \n",
    "        protect_train = df_train[protect_list[0]].values\n",
    "        protect_test = df_test[protect_list[0]].values\n",
    "        \n",
    "        ## socre discrimination with nn\n",
    "        pred_protct_train, pred_protect_test = score_dscr(score_train, protect_train, score_test, protect_test, hps[\"hps_score_dscr\"])\n",
    "        df_i_test[\"_my_infer_protect\"] = pred_protect_test\n",
    "        \n",
    "        ## score discrimination with random fortest\n",
    "        pred_protct_train, pred_protect_test = score_dscr_rf(score_train, protect_train, score_test, protect_test, hps[\"hps_score_dscr\"])\n",
    "        df_i_test[\"_my_infer_protect_rf\"] = pred_protect_test        \n",
    "        \n",
    "        ## score discrimination with naive bayes ber\n",
    "        pred_protct_train, pred_protect_test = score_dscr_nb_ber(score_train, protect_train, score_test, protect_test, hps[\"hps_score_dscr\"])\n",
    "        df_i_test[\"_my_infer_protect_nb_ber\"] = pred_protect_test \n",
    "        \n",
    "        if(False):\n",
    "            print(df_train.shape, df_test.shape)\n",
    "            print(score_train.shape, score_test.shape)\n",
    "            print(pred_protct_train.shape, pred_protect_test.shape)\n",
    "        \n",
    "        df_i_test[outcome] = df_test[outcome].values\n",
    "        df_i_test[result_name] = score_test\n",
    "        for column in protect_list:\n",
    "            df_i_test[column] = df_test[column].values    \n",
    "        df_res_test = df_res_test.append(df_i_test, ignore_index = True)\n",
    "        \n",
    "    return df_res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use a discriminator (MLP) to infer proetected feature from prediting scores\n",
    "\n",
    "def score_dscr(x_train, y_train, x_test, y_test, hps):\n",
    "    n_train = x_train.shape[0]\n",
    "    n_test = x_test.shape[0]\n",
    "    \n",
    "    if(False): \n",
    "        print(hps)\n",
    "        print(y_train)\n",
    "        \n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes = hps[\"hidden\"],\n",
    "        activation = \"relu\",\n",
    "        solver = \"adam\",\n",
    "        max_iter = 800,\n",
    "        #verbose = True,\n",
    "        )\n",
    "    clf.fit(x_train.reshape(n_train,1), y_train)\n",
    "    \n",
    "    pred_train = clf.predict(x_train.reshape(n_train,1))\n",
    "    pred_test = clf.predict(x_test.reshape(n_test,1))\n",
    "    \n",
    "    if(hps[\"verbos\"]):\n",
    "        acc_train = accuracy_score(y_train, pred_train)\n",
    "        acc_test = accuracy_score(y_test, pred_test)\n",
    "        print(\"NN disriminator acc on train/test %4f/%4f\"%(acc_train, acc_test))\n",
    "    \n",
    "    return pred_train, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use a discriminator (RF) to infer proetected feature from prediting scores\n",
    "\n",
    "def score_dscr_rf(x_train, y_train, x_test, y_test, hps):\n",
    "    n_train = x_train.shape[0]\n",
    "    n_test = x_test.shape[0]\n",
    "    \n",
    "    if(False):\n",
    "        print(hps)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=hps[\"rf_nes\"], max_depth=hps[\"rf_maxd\"])\n",
    "    clf.fit(x_train.reshape(n_train,1), y_train)\n",
    "    \n",
    "    pred_train = clf.predict(x_train.reshape(n_train,1))\n",
    "    pred_test = clf.predict(x_test.reshape(n_test,1))\n",
    "    \n",
    "    if(hps[\"verbos\"]):\n",
    "        acc_train = accuracy_score(y_train, pred_train)\n",
    "        acc_test = accuracy_score(y_test, pred_test)\n",
    "        print(\"RF disriminator acc on train/test %4f/%4f\"%(acc_train, acc_test))\n",
    "    \n",
    "    return pred_train, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def score_dscr_nb_ber(x_train, y_train, x_test, y_test, hps):\n",
    "    n_train = x_train.shape[0]\n",
    "    n_test = x_test.shape[0]\n",
    "    \n",
    "    if(False):\n",
    "        print(hps)  \n",
    "        \n",
    "    clf = BernoulliNB(binarize=0.5)\n",
    "    clf.fit(x_train.reshape(n_train,1), y_train)\n",
    "    pred_train = clf.predict(x_train.reshape(n_train,1))\n",
    "    pred_test = clf.predict(x_test.reshape(n_test,1))\n",
    "    \n",
    "    if(hps[\"verbos\"]):\n",
    "        acc_train = accuracy_score(y_train, pred_train)\n",
    "        acc_test = accuracy_score(y_test, pred_test)\n",
    "        print(\"BernoulliNB disriminator acc on train/test %4f/%4f\"%(acc_train, acc_test))\n",
    "    \n",
    "    return pred_train, pred_test    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_lbd(data_path, test_folder, feature_list, protect_list, outcome, lbd, hps, n_loop, save_score=False, save_path=None):\n",
    "    df_res_test = cross_validation(data_path, test_folder, feature_list, protect_list, outcome, lbd, hps, 5, \"pred_score\")\n",
    "    res_test = evaluate_utils.do_evaluate_score(df_res_test, \"pred_score\", outcome, protect_list, hps[\"nbins\"])\n",
    "    if(save_score):\n",
    "        df_res_test.to_csv(save_path)    \n",
    "    return res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./dataset/adult/\"\n",
    "feature_list = [\"x%d\"%(i) for i in range(103)] + [\"s\"]\n",
    "protect_list = [\"s\"]\n",
    "outcome = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameter tune\n",
    "if(False):\n",
    "    lbd = 1.0\n",
    "    hps_score_dscr = {\"hidden\":(64,64,64), \"rf_nes\":50, \"rf_maxd\":3, \"verbos\":False}\n",
    "    hps = {'C':-1, \"hps_score_dscr\":hps_score_dscr, \"nbins\":10}\n",
    "    n_loop = -1\n",
    "    c_list = [1e-4,1e-2,1e0,1e2,1e4,1e6,1e8,1e10,1e20]\n",
    "    print(\"C, acc_y, acc_p, acc_p_rf, corr, f1_y, f1_p\")\n",
    "    for c in c_list:\n",
    "        hps[\"C\"] = c\n",
    "        res_test = evaluate_single_lbd(data_path, \"valid\", feature_list, protect_list, outcome, lbd, hps, n_loop)\n",
    "        #print(\"c = %e, acc_y = %4f, acc_protect = %4f\"%(c,res_test[\"acc\"],res_test[\"acc_infer_protect_cross\"]))\n",
    "        print(\"%4e %4f %4f %4f %4f %4f %4f\"%(c, res_test[\"acc_y\"], res_test[\"acc_p\"], res_test[\"acc_p_rf\"], res_test[\"corr\"], res_test[\"f1_y\"], res_test[\"f1_p\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fine tuning\n",
    "if(False):\n",
    "    lbd = 1.0\n",
    "    hps_score_dscr = {\"hidden\":(64,64,64), \"rf_nes\":50, \"rf_maxd\":3, \"verbos\":False}\n",
    "    hps = {'C':-1, \"hps_score_dscr\":hps_score_dscr, \"nbins\":10}\n",
    "    n_loop = -1\n",
    "\n",
    "    c_list = [1e-1,2e-1,5e-1,1e0,2e0,5e0,1e1,2e1,5e1,1e2,2e2,5e2,1e3]\n",
    "    print(\"C, acc_y, acc_p, acc_p_rf, corr, f1_y, f1_p\")\n",
    "    for c in c_list:\n",
    "        hps[\"C\"] = c\n",
    "        res_test = evaluate_single_lbd(data_path, \"valid\", feature_list, protect_list, outcome, lbd, hps, n_loop)\n",
    "        #print(\"c = %e, acc_y = %4f, acc_protect = %4f\"%(c,res_test[\"acc\"], res_test[\"acc_infer_protect_cross\"]))\n",
    "        print(\"%4e %4f %4f %4f %4f %4f %4f\"%(c, res_test[\"acc_y\"], res_test[\"acc_p\"], res_test[\"acc_p_rf\"], res_test[\"corr\"], res_test[\"f1_y\"], res_test[\"f1_p\"]))\n",
    "## C, acc_y, acc_p, corr, f1_y, f1_p\n",
    "## 1.000000e+04 0.850290 0.673705 -0.286733 0.668623 0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_y = 0.843665, acc_protect = 0.673081/0.673772, corr = -0.100952\n"
     ]
    }
   ],
   "source": [
    "## test on optimal hyper parameters\n",
    "importlib.reload(evaluate_utils)\n",
    "lbd = 0.0\n",
    "hps_score_dscr = {\"hidden\":(64,64,64), \"rf_nes\":50, \"rf_maxd\":3, \"verbos\":False}\n",
    "hps = {'C':2e0, \"hps_score_dscr\":hps_score_dscr, \"nbins\":10}\n",
    "n_loop = -1\n",
    "res_test = evaluate_single_lbd(data_path, \"test\", feature_list, protect_list, outcome, lbd, hps, n_loop)\n",
    "print((\"acc_y = %4f, acc_protect = %4f/%4f, corr = %4f\")%(res_test[\"acc_y\"], res_test[\"acc_p\"], res_test[\"acc_p_rf\"], res_test[\"corr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "importlib.reload(evaluate_utils)\n",
    "evaluate_result = {}\n",
    "lbd_list = np.arange(0.0,1.05,0.05)\n",
    "\n",
    "os.system(\"rm -rf ./pred_scores_model0; mkdir ./pred_scores_model0\")\n",
    "i = -1\n",
    "for lbd in lbd_list:\n",
    "    i+=1\n",
    "    save_path = \"./pred_scores_model0/pred_%d.csv\"%(i) \n",
    "    \n",
    "    hps_score_dscr = {\"hidden\":(64,64,64), \"rf_nes\":50, \"rf_maxd\":3, \"verbos\":False}\n",
    "    hps = {'C':2e0, \"hps_score_dscr\":hps_score_dscr, \"nbins\":10}\n",
    "    n_loop = -1\n",
    "    res_test = evaluate_single_lbd(data_path, \"test\", feature_list, protect_list, outcome, lbd, hps, n_loop, save_score=True, save_path=save_path)\n",
    "    print((\"lbd = %4f, acc_y = %4f, acc_protect = %4f/%4f, corr = %4f\")%(lbd, res_test[\"acc_y\"], res_test[\"acc_p\"], res_test[\"acc_p_rf\"], res_test[\"corr\"]))    #print(\"%4f %4f %4f %4f\"%(lbd, res_test[\"acc_y\"], res_test[\"acc_p\"], res_test[\"corr\"]))\n",
    "    \n",
    "    res_test[\"lbd\"] = lbd\n",
    "    for key in res_test:\n",
    "        if(key in evaluate_result): evaluate_result[key].append(res_test[key])\n",
    "        else: evaluate_result[key] = [res_test[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adult_model0_evaluate.txt', 'w') as outfile:\n",
    "    json.dump(evaluate_result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
